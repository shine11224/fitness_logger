import streamlit as st
import openai
import json
import pandas as pd
from datetime import datetime
import mysql.connector
import requests
import PyPDF2
import tiktoken
import os

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(page_title="pdf_management", page_icon="ğŸ“•", layout="wide")

# --- 2. æ ¸å¿ƒå·¥å…·åº“ (åŸºç¡€è®¾æ–½) ---

# A. åˆå§‹åŒ– API
if "DEEPSEEK_API_KEY" not in st.secrets:
    st.error("âŒ æœªæ‰¾åˆ° API Keyï¼Œè¯·åœ¨ secrets.toml ä¸­é…ç½®")
    st.stop()

client = openai.Client(
    api_key=st.secrets["DEEPSEEK_API_KEY"],
    base_url="https://api.deepseek.com/v1"
)


# B. æ•°æ®åº“è¿æ¥
def get_db_connection():
    return mysql.connector.connect(
        host=st.secrets["tidb"]["host"],
        port=st.secrets["tidb"]["port"],
        user=st.secrets["tidb"]["user"],
        password=st.secrets["tidb"]["password"],
        database=st.secrets["tidb"]["database"]
    )


# C. æ•°æ®åº“å†™å…¥ (è·¯ç”±åˆ†å‘)
def save_to_db(table_name, data_dict):
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if table_name == "paper_notes":
            # å¤„ç†åˆ—è¡¨è½¬å­—ç¬¦ä¸²
            tags_str = ",".join(data_dict.get('tags', []))
            file_path = data_dict.get('file_path', '')
            summary = data_dict.get('summary', '')  # è·å–æ™ºèƒ½æ‘˜è¦

            sql = "INSERT INTO paper_notes (paper_name, question, answer, tags, file_path, summary, log_time) VALUES (%s, %s, %s, %s, %s, %s, %s)"
            val = (data_dict['paper_name'], data_dict['question'], data_dict['answer'], tags_str, file_path, summary,
                   current_time)
        cursor.execute(sql, val)
        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        st.error(f"âŒ TiDB å†™å…¥å¤±è´¥: {e}")
        return False


def load_from_db(table_name):
    try:
        conn = get_db_connection()
        query = f"SELECT * FROM {table_name} ORDER BY log_time DESC"  # é»˜è®¤å€’åº
        df = pd.read_sql(query, conn)
        conn.close()
        return df
    except Exception as e:
        # st.error(f"è¯»å–æ•°æ®å¤±è´¥: {e}") # ç”Ÿäº§ç¯å¢ƒå¯ä»¥æ³¨é‡Šæ‰ä»¥å…å¹²æ‰°
        return pd.DataFrame()


# D. é£ä¹¦åŒæ­¥
def get_feishu_token():
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    req = {
        "app_id": st.secrets["feishu"]["app_id"],
        "app_secret": st.secrets["feishu"]["app_secret"]
    }
    resp = requests.post(url, json=req).json()
    return resp.get("tenant_access_token")


def save_to_feishu(type_key, data):
    try:
        token = get_feishu_token()
        if not token: return False
        app_token = st.secrets["feishu"]["app_token"]
        if type_key == "paper":
            table_id = st.secrets["feishu"]["paper_table_id"]
            tags_str = ",".join(data.get('tags', []))
            fields = {
                "æ–‡çŒ®å": data['paper_name'],
                "é—®é¢˜": data['question'],
                "AIè§£è¯»": data['answer'],
                "æ ‡ç­¾": tags_str,
                "ç²¾ç®€æ‘˜è¦": data.get('summary', ''),
                "è®°å½•æ—¶é—´": int(datetime.now().timestamp() * 1000)
            }

        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        payload = {"fields": fields}
        resp = requests.post(url, headers=headers, json=payload).json()
        return resp.get("code") == 0
    except Exception as e:
        st.error(f"âŒ é£ä¹¦åŒæ­¥å¤±è´¥: {e}")
        return False


# E. æœ¬åœ°æ–‡ä»¶ç®¡ç†
def save_uploaded_file(uploaded_file):
    library_dir = "paper_library"
    if not os.path.exists(library_dir):
        os.makedirs(library_dir)
    file_path = os.path.join(library_dir, uploaded_file.name)
    # 4. ã€æ–°å¢åŠŸèƒ½ã€‘æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨
    if os.path.exists(file_path):
        # å¦‚æœå­˜åœ¨ï¼Œç›´æ¥è¿”å›è·¯å¾„ï¼Œå¹¶æ ‡è®° is_new = False
        return file_path, False

    # 5. å¦‚æœä¸å­˜åœ¨ï¼Œæ‰è¿›è¡Œå†™å…¥æ“ä½œ
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # è¿”å›è·¯å¾„ï¼Œå¹¶æ ‡è®° is_new = True (ä»£è¡¨æ˜¯æ–°å­˜çš„)
    return file_path, True



# F. Token è®¡æ•°
def count_tokens(text):
    encoding = tiktoken.get_encoding("cl100k_base")
    return len(encoding.encode(text))


# --- åŠŸèƒ½æ¨¡å— æ–‡çŒ®é˜…è¯»ï¼š
@st.cache_data
def extract_text_from_pdf(uploaded_file):
    pdf_reader = PyPDF2.PdfReader(uploaded_file)
    text = ""
    for i, page in enumerate(pdf_reader.pages):
        c = page.extract_text()
        if c: text += f"\n\n--- [ç¬¬ {i + 1} é¡µ] ---\n\n{c}"
    return text


def render_med_reader():
    st.header("ğŸ“„ AI æ–‡çŒ®é˜…è¯»åŠ©æ‰‹ (Pro)")
    st.caption("RAG é˜…è¯» | æ ‡ç­¾ç®¡ç† | å­˜ç®—åˆ†ç¦»æ¶æ„")
    if "init_done" not in st.session_state:
        st.session_state.chat_history = []
        default_tags = []
        # ä»æ•°æ®åº“ææ ‡ç­¾
        df_history = load_from_db("paper_notes")
        db_tags = set()
        if not df_history.empty and 'tags' in df_history.columns:
            for tag_str in df_history['tags']:
                if tag_str:
                    for p in tag_str.split(","):
                        if p.strip(): db_tags.add(p.strip())

        all_tags = list(set(default_tags + list(db_tags)))
        all_tags.sort()
        st.session_state.all_tags = all_tags
        st.session_state.init_done = True

    # 1. æ–‡ä»¶ä¸Šä¼ åŒº
    with st.sidebar:
        st.markdown("### ğŸ“¥ æ–‡çŒ®ä¸Šä¼ ")
        uploaded_file = st.file_uploader("Upload PDF", type="pdf")
        if uploaded_file:
            # è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°ä¹¦æ¶
            saved_path = save_uploaded_file(uploaded_file)
            st.session_state.current_file_path = saved_path

            # æ¸…ç©ºæ—§ä¼šè¯
            if "last_file" not in st.session_state or st.session_state.last_file != uploaded_file.name:
                st.session_state.chat_history = []
                st.session_state.last_file = uploaded_file.name
                st.toast("æ–°æ–‡çŒ®å·²åŠ è½½ï¼Œè®°å¿†é‡ç½®")

            # æå–æ–‡æœ¬
            paper_text = extract_text_from_pdf(uploaded_file)
            tokens = count_tokens(paper_text)
            st.success(f"å·²è§£æ: {len(paper_text)} å­—ç¬¦")
            st.caption(f"Token ä¼°ç®—: {tokens}")
            if len(paper_text) > 2000:
                # å¦‚æœæ–‡ç« å¾ˆé•¿ï¼Œæ˜¾ç¤ºå¤´å°¾
                preview_content = paper_text[:1000] + "\n\n... (ä¸­é—´å†…å®¹å·²çœç•¥) ...\n\n" + paper_text[-1000:]
            else:
                # å¦‚æœæ–‡ç« æœ¬èº«å°±ä¸é•¿ï¼Œç›´æ¥æ˜¾ç¤ºå…¨éƒ¨
                preview_content = paper_text
            with st.expander("ç‚¹å‡»å±•å¼€æŸ¥çœ‹æ–‡æ¡£é¢„è§ˆ"):
                st.markdown(preview_content)
    # 2. èŠå¤©äº¤äº’åŒº
    if uploaded_file and 'paper_text' in locals():
        for msg in st.session_state.chat_history:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

        query = st.chat_input("å…³äºè¿™ç¯‡è®ºæ–‡ï¼Œä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ")
        if query:
            with st.chat_message("user"):
                st.write(query)
            st.session_state.chat_history.append({"role": "user", "content": query})

            # æ„é€ å¸¦ç¼“å­˜çš„æ¶ˆæ¯é“¾
            messages = [
                           {"role": "system",
                            "content": f"""
                            ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŒ»å­¦ç§‘ç ”åŠ©æ‰‹ã€‚
                             1. è¯·åŸºäºæˆ‘æä¾›çš„ã€è®ºæ–‡å†…å®¹ã€‘å›ç­”é—®é¢˜ã€‚
                             2. **å¿…é¡»å¼•ç”¨åŸæ–‡**ï¼šåœ¨å›ç­”çš„å…³é”®è§‚ç‚¹åï¼Œè¯·æ ‡æ³¨å‡ºå¤„ï¼Œä¾‹å¦‚ (è§ç¬¬ 3 é¡µ)ã€‚
                             3. å¦‚æœè®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”â€œæ–‡ä¸­æœªæåŠâ€ï¼Œä¸è¦ç¼–é€ ã€‚
                             4. ä¿æŒå›ç­”çš„é€»è¾‘æ€§ï¼Œä½¿ç”¨ Markdown æ ¼å¼ï¼ˆå¦‚åˆ—è¡¨ã€ç²—ä½“ï¼‰ã€‚
                            ã€è®ºæ–‡å…¨æ–‡ã€‘ï¼š
                            {paper_text}
                            """},
                       ]
            messages.extend(st.session_state.chat_history)
            with st.chat_message("assistant"):
                with st.spinner("AI æ€è€ƒä¸­..."):
                    try:
                        resp = client.chat.completions.create(
                            model="deepseek-chat", messages=messages, temperature=0.1
                        )
                        ans = resp.choices[0].message.content
                        st.markdown(ans)
                        st.session_state.chat_history.append({"role": "assistant", "content": ans})

                        # è´¹ç”¨ç»Ÿè®¡
                        if resp.usage:
                            prompt_tokens = resp.usage.prompt_tokens  # æé—®æ¶ˆè€— (PDF + é—®é¢˜)
                            completion_tokens = resp.usage.completion_tokens  # å›ç­”æ¶ˆè€— (AI å†™çš„å­—)
                            # ç¼“å­˜å‘½ä¸­çš„ Token æ•°é‡ (Cache Hit)
                            cached_tokens = resp.usage.prompt_cache_hit_tokens
                            # å®é™…æ‰£è´¹çš„ Token æ•°é‡ (Cache Miss)
                            miss_tokens = resp.usage.prompt_cache_miss_tokens
                            total = resp.usage.total_tokens

                            st.caption(f"""
                            ğŸ’° **DeepSeek ç¼“å­˜ç»Ÿè®¡**:
                            - ğŸ“¥ é˜…è¯» (Input): `{prompt_tokens}` Tokens
                            - âœ… å‘½ä¸­ç¼“å­˜: `{cached_tokens}` Tokens 
                            - ğŸ†• æ–°å¢è¯»å–: `{miss_tokens}` Tokens 
                            - ğŸ“¤ æ€è€ƒ (Output): `{completion_tokens}` Tokens
                            - ğŸ’° æ€»è®¡ (Total): `{total}` Tokens
                            """)
                    except Exception as e:
                        st.error(f"Error: {e}")

    #  3. ç¬”è®°ä¿å­˜åŒº (å‡çº§ç‰ˆï¼šæ”¯æŒè‡ªå®šä¹‰æ ‡ç­¾)
        st.divider()
        if len(st.session_state.chat_history) > 0:
            with st.expander("ğŸ’¾ ä¿å­˜å½“å‰å¯¹è¯åˆ°ç¬”è®°", expanded=True):
                with st.form("save_note"):
                    # è·å–æœ€è¿‘ä¸€æ¬¡é—®ç­”
                    last_q = st.session_state.chat_history[-2]['content']
                    last_a = st.session_state.chat_history[-1]['content']
                    p_name = st.session_state.get("last_file", "æœªçŸ¥")

                    # æ ‡ç­¾ç³»ç»Ÿ
                    sel_tags = st.multiselect("å·²æœ‰æ ‡ç­¾", options=st.session_state.all_tags)
                    new_tags = st.text_input("æ–°å¢æ ‡ç­¾", placeholder="ä¾‹å¦‚ï¼šç½•è§ç—…, åŸºå› ç¼–è¾‘")

                    if st.form_submit_button("âœ… ç¡®è®¤å½’æ¡£"):
                        # åˆå¹¶æ ‡ç­¾
                        custom = [t.strip() for t in new_tags.split(",") if t.strip()]
                        final_tags = list(set(sel_tags + custom))

                        # å­¦ä¹ æ–°æ ‡ç­¾
                        for t in custom:
                            if t not in st.session_state.all_tags:
                                st.session_state.all_tags.append(t)

                         # ç”Ÿæˆ One-Liner æ‘˜è¦
                        summary_text = ""
                        with st.spinner("æ­£åœ¨ç”Ÿæˆç²¾ç®€æ‘˜è¦..."):
                            try:
                                sum_resp = client.chat.completions.create(
                                model="deepseek-chat",
                                messages=[{"role": "user",
                                                   "content": f"è¯·ä¸ºä»¥ä¸‹é—®ç­”ç”Ÿæˆä¸€ä¸ª20å­—ä»¥å†…çš„æ ¸å¿ƒç»“è®ºæ‘˜è¦ï¼Œä¸è¦æ ‡ç‚¹ï¼š\né—®ï¼š{last_q}\nç­”ï¼š{last_a}"}],
                                            temperature=0.1
                                        )
                                summary_text = sum_resp.choices[0].message.content.strip()
                            except:
                                summary_text = "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
                                # å­˜åº“
                            data = {
                                "paper_name": p_name, "question": last_q, "answer": last_a,
                                "tags": final_tags, "summary": summary_text,
                                "file_path": st.session_state.get("current_file_path", "")
                                }

                            if save_to_db("paper_notes", data) and save_to_feishu("paper", data):
                                st.success(f"å·²å½’æ¡£ï¼æ‘˜è¦: {summary_text}")

                            else:
                                 st.error("ä¿å­˜å¤±è´¥")

    # 4. çŸ¥è¯†åº“æµè§ˆåŒº (åˆ†æ  + äº¤äº’ + ä¸‹è½½)
    st.header("ğŸ“š ç§‘ç ”çŸ¥è¯†åº“")
    df = load_from_db("paper_notes")

    if not df.empty:
        # æ•°æ®æ¸…æ´—
        if 'tags' not in df.columns: df['tags'] = ""
        df['tags'] = df['tags'].fillna("")
        if 'summary' not in df.columns: df['summary'] = ""
        df['summary'] = df['summary'].fillna("æ— æ‘˜è¦")

        # æ ‡ç­¾ç­›é€‰å™¨
        all_db_tags = set(t for s in df['tags'] for t in s.split(",") if t)
        with st.expander("ğŸ” ç­›é€‰ä¸å¯¼å‡º"):
            col_f1, col_f2 = st.columns(2)
            filter_tags = col_f1.multiselect("æŒ‰æ ‡ç­¾ç­›é€‰", list(all_db_tags))

            # å¯¼å‡º CSV
            csv = df.to_csv(index=False).encode('utf-8-sig')
            col_f2.download_button("ğŸ“¤ å¯¼å‡ºå¤‡ä»½ (CSV)", csv, "medical_notes.csv", "text/csv")

        # åº”ç”¨ç­›é€‰
        if filter_tags:
            df = df[df['tags'].apply(lambda x: any(t in x.split(",") for t in filter_tags))]

        # åˆ†æ å¸ƒå±€
        col_list, col_detail = st.columns([2, 3])

        with col_list:
            st.caption(f"å…± {len(df)} æ¡ç¬”è®°")
            # äº¤äº’å¼è¡¨æ ¼
            event = st.dataframe(
                df,
                column_config={
                    "id": st.column_config.NumberColumn("ID", width="small"),
                    "summary": st.column_config.TextColumn("ğŸ“Œ æ ¸å¿ƒç»“è®º", width="large"),
                    "tags": st.column_config.TextColumn("æ ‡ç­¾", width="medium"),
                    "paper_name": st.column_config.Column(hidden=True),
                    "question": st.column_config.Column(hidden=True),
                    "answer": st.column_config.Column(hidden=True),
                    "file_path": st.column_config.Column(hidden=True)
                },
                use_container_width=True, hide_index=True, selection_mode="single-row", on_select="rerun", height=600
            )

        with col_detail:
            if len(event.selection.rows) > 0:
                row = df.iloc[event.selection.rows[0]]
                with st.container(border=True):
                    # è¯¦æƒ…å¡ç‰‡
                    st.markdown(f"### ğŸ“„ {row['paper_name']}")
                    if row['tags']:
                        # æ¸²æŸ“æ ‡ç­¾
                        tags_html = " ".join([f"`{t}`" for t in row['tags'].split(",") if t])
                        st.markdown(f"ğŸ·ï¸ {tags_html}")

                    st.divider()
                    st.info(f"ğŸ“Œ **æ‘˜è¦**: {row['summary']}")
                    st.markdown("#### â“ é—®é¢˜");
                    st.write(row['question'])
                    st.markdown("#### ğŸ¤– è§£è¯»");
                    st.markdown(row['answer'])

                    st.divider()
                    # åŸå§‹æ–‡ä»¶ä¸‹è½½
                    if row['file_path'] and os.path.exists(row['file_path']):
                        with open(row['file_path'], "rb") as f:
                            st.download_button("ğŸ“¥ æ‰“å¼€/ä¸‹è½½åŸå§‹ PDF", f, file_name=row['paper_name'])
                    else:
                        st.caption("âš ï¸ åŸå§‹æ–‡ä»¶æœªåœ¨æœ¬åœ°æ‰¾åˆ° (ä»…æ˜¾ç¤ºäº‘ç«¯ç¬”è®°)")
            else:
                st.info("ğŸ‘ˆ è¯·ç‚¹å‡»å·¦ä¾§åˆ—è¡¨æŸ¥çœ‹è¯¦æƒ…")
# --- 5. ä¸»ç¨‹åºå…¥å£ ---
def main():
    if __name__ == "__main__":
        main()